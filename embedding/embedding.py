"""
This script converts the csv data that 'data.py' generates into '.npy' files
"""

import fasttext
import os
import csv
import numpy as np


# Preprocessed data folders paths
preprocessed_data_paths = {
    'training path':    '..\\data\\preprocessed data\\training',
    'validation path':  '..\\data\\preprocessed data\\validation',
    'test path':        '..\\data\\preprocessed data\\test'
}


# 'text9' is cleaned the data of the English 'Wikipedia 9' dataset: enwik9. The data is UTF-8 encoded XML consisting
# primarily of English text. enwik9 contains 243,426 article titles, of which 85,560 are #REDIRECT to fix broken links,
# and the rest are regular articles.
wiki_dataset = '..\\..\\fasttextWiki\\text9'

skipgram_path = 'model_skipgram.bin'

seq_dim = 250  # size of embedded vector
output_dim_rumors = 3
output_dim_stances = 4

# labels to np.array
# for rumors
label_true = np.array([1.0, 0.0, 0.0], dtype=np.float32)
label_false = np.array([0.0, 1.0, 0.0], dtype=np.float32)
label_unverified = np.array([0.0, 0.0, 1.0], dtype=np.float32)
# for stances
label_support = np.array([1.0, 0.0, 0.0, 0.0], dtype=np.float32)
label_deny = np.array([0.0, 1.0, 0.0, 0.0], dtype=np.float32)
label_query = np.array([0.0, 0.0, 1.0, 0.0], dtype=np.float32)
label_comment = np.array([0.0, 0.0, 0.0, 1.0], dtype=np.float32)


def prepare_data(base_path, csv_path, output_dim, fasttext_model, counters):
    """
    Gets CSV with tweets and labels and creates np.arrays with embedded tweets and labels
    :param base_path:       path to the folder
    :param csv_path:        path to the csv file
    :param output_dim:      will be 3 for rumor detection and 4 for stance classification
    :param fasttext_model:  the skip-gram model generated by FastText
    :param counters:        dictionary holding 2 counters, one for each task
    :return: void
    """
    with open(csv_path, 'r') as csv_file:
        num_of_tweets = sum(1 for line in csv_file)
        num_of_tweets -= 1

    with open(csv_path, 'r') as csv_file:
        csv_reader = csv.reader(csv_file, delimiter=',')
        next(csv_reader, None)  # skip the heading

        if output_dim == output_dim_rumors:
            set_name = 'rumors'
        else:
            set_name = 'stances'

        tweets = np.zeros((num_of_tweets, seq_dim), dtype=np.float32)
        labels = np.zeros((num_of_tweets, output_dim), dtype=np.float32)

        for i, row in enumerate(csv_reader):
            tweets[i, :] = fasttext_model.get_sentence_vector(row[0])
            label = row[1].lower()

            if set_name == 'rumors':
                counters['rumors'] += 1
                if label == 'true':
                    labels[i, :] = label_true
                elif label == 'false':
                    labels[i, :] = label_false
                elif label == 'unverified':
                    labels[i, :] = label_unverified
            else:
                counters['stances'] += 1
                if label == 'support':
                    labels[i, :] = label_support
                elif label == 'deny':
                    labels[i, :] = label_deny
                elif label == 'query':
                    labels[i, :] = label_query
                elif label == 'comment':
                    labels[i, :] = label_comment

        np.save(os.path.join(base_path, set_name + '_tweets.npy'), tweets)
        np.save(os.path.join(base_path, set_name + '_labels.npy'), labels)


def main():
    if not os.path.isfile(skipgram_path):
        model = fasttext.train_unsupervised(wiki_dataset,
                                            "skipgram",
                                            minn=3,  # min length of char ngram - subword between 3 and 6 characters is as popular
                                            maxn=6,  # max length of char ngram
                                            dim=seq_dim,  # size of word vectors - any value in the 100-300 range is as popular
                                            lr=0.1  # learning rate - The default value is 0.05 which is a good compromise. If you want to play with it we suggest to stay in the range of [0.01, 1]
                                            )
        model.save_model(skipgram_path)
    else:
        model = fasttext.load_model(skipgram_path)

    # go through the dataset and use fasttest (for embedding) to create numpy arrays
    counters = {
        'rumors': 0,
        'stances': 0,
    }

    # go through training data folder, validation data folder and test data folder
    for _, preprocessed_data_path in preprocessed_data_paths.items():
        counters['rumors'] = 0
        counters['stances'] = 0

        rumors_csv_path = os.path.join(preprocessed_data_path, 'rumors.csv')
        stances_csv_path = os.path.join(preprocessed_data_path, 'stances.csv')

        prepare_data(preprocessed_data_path, rumors_csv_path, output_dim_rumors, model, counters)
        prepare_data(preprocessed_data_path, stances_csv_path, output_dim_stances, model, counters)

        # print counters
        if 'training' in preprocessed_data_path:
            set_name = 'training'
        elif 'validation' in preprocessed_data_path:
            set_name = 'validation'
        else:
            set_name = 'test'
        print(set_name + ': ' + str(counters))


if __name__ == '__main__':
    main()
    print('done')
